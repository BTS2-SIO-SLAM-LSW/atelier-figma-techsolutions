#!/usr/bin/env python3
"""
Tests automatiques pour l'Atelier Figma TechSolutions
GitHub Classroom Autograding
"""

import json
import sys
import argparse
import os
import re
from pathlib import Path
import requests
from jsonschema import validate, ValidationError

class FigmaWorkshopTester:
    def __init__(self):
        self.submission_path = "submissions/my_submission.json"
        self.template_path = "submissions/student_template.json"
        self.errors = []
        self.warnings = []
        self.score = 0
        self.max_score = 0

    def load_submission(self):
        """Charge le fichier de soumission √©tudiant"""
        if not os.path.exists(self.submission_path):
            raise FileNotFoundError(f"‚ùå Fichier de soumission introuvable: {self.submission_path}")
        
        try:
            with open(self.submission_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"‚ùå Fichier JSON invalide: {e}")

    def load_template(self):
        """Charge le template pour validation du sch√©ma"""
        try:
            with open(self.template_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return None

    def test_json_structure(self):
        """TEST 1: Validation de la structure JSON (20 points)"""
        print("\nüîç TEST 1 - Structure JSON")
        self.max_score = 20
        
        try:
            submission = self.load_submission()
            template = self.load_template()
            
            # V√©rifications de base
            required_sections = [
                "etudiant_info", "analyse_ux", "conception", 
                "prototypage", "qcm_responses", "auto_evaluation"
            ]
            
            for section in required_sections:
                if section not in submission:
                    self.errors.append(f"‚ùå Section manquante: {section}")
                else:
                    self.score += 3
                    print(f"‚úÖ Section pr√©sente: {section}")
            
            # Validation des champs √©tudiant
            if "etudiant_info" in submission:
                student_info = submission["etudiant_info"]
                required_fields = ["nom", "prenom", "email", "date_soumission"]
                
                for field in required_fields:
                    if field in student_info and student_info[field]:
                        self.score += 1
                        print(f"‚úÖ Info √©tudiant: {field}")
                    else:
                        self.errors.append(f"‚ùå Info manquante: {field}")
            
            # Score bonus pour structure propre
            if len(self.errors) == 0:
                self.score += 2
                print("‚úÖ Bonus: Structure parfaite")
            
        except Exception as e:
            self.errors.append(f"‚ùå Erreur structure: {e}")
        
        print(f"üìä Score structure: {self.score}/{self.max_score}")
        return self.score > 0

    def test_ux_content(self):
        """TEST 2: Validation du contenu UX obligatoire (30 points)"""
        print("\nüé® TEST 2 - Contenu UX")
        self.score = 0
        self.max_score = 30
        
        try:
            submission = self.load_submission()
            
            # Test analyse UX (10 points)
            if "analyse_ux" in submission:
                analyse = submission["analyse_ux"]
                
                # Personas d√©finis
                if "personas" in analyse and len(analyse["personas"]) >= 2:
                    self.score += 3
                    print("‚úÖ Personas d√©finis (2+ personas)")
                else:
                    self.errors.append("‚ùå Minimum 2 personas requis")
                
                # Cas d'utilisation
                if "cas_utilisation" in analyse and len(analyse["cas_utilisation"]) >= 3:
                    self.score += 3
                    print("‚úÖ Cas d'utilisation d√©finis (3+ cas)")
                else:
                    self.errors.append("‚ùå Minimum 3 cas d'utilisation requis")
                
                # Journey maps
                if "journey_maps" in analyse and analyse["journey_maps"]:
                    self.score += 4
                    print("‚úÖ Journey maps pr√©sents")
                else:
                    self.warnings.append("‚ö†Ô∏è Journey maps recommand√©s")
            
            # Test conception (10 points)
            if "conception" in submission:
                conception = submission["conception"]
                
                # Wireframes
                if "wireframes" in conception and len(conception["wireframes"]) >= 3:
                    self.score += 5
                    print("‚úÖ Wireframes pr√©sents (3+ √©crans)")
                else:
                    self.errors.append("‚ùå Minimum 3 wireframes requis")
                
                # Design system
                if "design_system" in conception:
                    ds = conception["design_system"]
                    if "colors" in ds and "typography" in ds:
                        self.score += 3
                        print("‚úÖ Design system d√©fini")
                    else:
                        self.errors.append("‚ùå Design system incomplet")
                else:
                    self.errors.append("‚ùå Design system manquant")
                
                # Composants UI
                if "composants_ui" in conception and len(conception["composants_ui"]) >= 5:
                    self.score += 2
                    print("‚úÖ Composants UI cr√©√©s")
            
            # Test prototypage (10 points)
            if "prototypage" in submission:
                proto = submission["prototypage"]
                
                # Interactions d√©finies
                if "interactions" in proto and len(proto["interactions"]) >= 5:
                    self.score += 5
                    print("‚úÖ Interactions d√©finies (5+)")
                else:
                    self.warnings.append("‚ö†Ô∏è Plus d'interactions recommand√©es")
                
                # Navigation coh√©rente
                if "navigation" in proto and proto["navigation"]:
                    self.score += 3
                    print("‚úÖ Navigation d√©finie")
                
                # Tests utilisateurs
                if "tests_utilisateurs" in proto and proto["tests_utilisateurs"]:
                    self.score += 2
                    print("‚úÖ Tests utilisateurs document√©s")
        
        except Exception as e:
            self.errors.append(f"‚ùå Erreur contenu UX: {e}")
        
        print(f"üìä Score contenu UX: {self.score}/{self.max_score}")
        return self.score >= 15  # 50% minimum

    def test_figma_links(self):
        """TEST 3: Validation des liens Figma (25 points)"""
        print("\nüîó TEST 3 - Liens Figma")
        self.score = 0
        self.max_score = 25
        
        try:
            submission = self.load_submission()
            figma_pattern = r'https://www\.figma\.com/(file|proto)/[A-Za-z0-9]+'
            
            # Recherche liens dans toute la soumission
            links_found = []
            
            def extract_links(obj, path=""):
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        if isinstance(value, str) and re.search(figma_pattern, value):
                            links_found.append(f"{path}.{key}" if path else key)
                        elif isinstance(value, (dict, list)):
                            extract_links(value, f"{path}.{key}" if path else key)
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        extract_links(item, f"{path}[{i}]" if path else f"[{i}]")
            
            extract_links(submission)
            
            # Validation des liens
            valid_links = 0
            for link_path in links_found:
                # Test simple de format (sans requ√™te HTTP pour √©viter rate limits)
                if "figma.com" in str(submission):
                    valid_links += 1
                    self.score += 5
                    print(f"‚úÖ Lien Figma trouv√©: {link_path}")
            
            # Bonus pour liens multiples
            if valid_links >= 2:
                self.score += 5
                print("‚úÖ Bonus: Liens multiples")
            
            # V√©rification sp√©cifique prototype
            if "prototypage" in submission and "lien_prototype" in submission["prototypage"]:
                proto_link = submission["prototypage"]["lien_prototype"]
                if re.search(figma_pattern, proto_link):
                    self.score += 10
                    print("‚úÖ Lien prototype valide")
                else:
                    self.errors.append("‚ùå Lien prototype invalide")
            else:
                self.errors.append("‚ùå Lien prototype manquant")
        
        except Exception as e:
            self.errors.append(f"‚ùå Erreur liens Figma: {e}")
        
        print(f"üìä Score liens Figma: {self.score}/{self.max_score}")
        return self.score >= 10  # Minimum 40%

    def test_qcm_answers(self):
        """TEST 4: Validation r√©ponses QCM (15 points)"""
        print("\nüìù TEST 4 - R√©ponses QCM")
        self.score = 0
        self.max_score = 15
        
        try:
            submission = self.load_submission()
            
            if "qcm_responses" not in submission:
                self.errors.append("‚ùå Section qcm_responses manquante")
                print(f"üìä Score QCM: {self.score}/{self.max_score}")
                return False
            
            qcm = submission["qcm_responses"]
            expected_phases = ["phase1", "phase2", "phase3", "final"]
            
            for phase in expected_phases:
                if phase in qcm:
                    phase_answers = qcm[phase]
                    if isinstance(phase_answers, dict) and len(phase_answers) > 0:
                        self.score += 3
                        print(f"‚úÖ QCM {phase}: r√©ponses pr√©sentes")
                        
                        # V√©rification que les r√©ponses ne sont pas vides
                        valid_answers = sum(1 for v in phase_answers.values() if v is not None and v != "")
                        if valid_answers >= len(phase_answers) * 0.8:  # 80% des r√©ponses
                            self.score += 1
                            print(f"‚úÖ QCM {phase}: r√©ponses compl√®tes")
                    else:
                        self.errors.append(f"‚ùå QCM {phase}: r√©ponses manquantes")
                else:
                    self.errors.append(f"‚ùå QCM {phase}: section manquante")
            
            # Bonus pour justifications
            if any("justification" in str(phase_data) for phase_data in qcm.values()):
                self.score += 3
                print("‚úÖ Bonus: Justifications pr√©sentes")
        
        except Exception as e:
            self.errors.append(f"‚ùå Erreur QCM: {e}")
        
        print(f"üìä Score QCM: {self.score}/{self.max_score}")
        return self.score >= 8  # Minimum 50%

    def test_self_evaluation(self):
        """TEST 5: Auto-√©valuation compl√®te (10 points)"""
        print("\nüìã TEST 5 - Auto-√©valuation")
        self.score = 0
        self.max_score = 10
        
        try:
            submission = self.load_submission()
            
            if "auto_evaluation" not in submission:
                self.errors.append("‚ùå Section auto_evaluation manquante")
                print(f"üìä Score auto-√©valuation: {self.score}/{self.max_score}")
                return False
            
            auto_eval = submission["auto_evaluation"]
            
            # Crit√®res obligatoires
            required_criteria = [
                "analyse_ux_score", "conception_score", "prototypage_score",
                "points_forts", "points_amelioration", "satisfaction_globale"
            ]
            
            for criterion in required_criteria:
                if criterion in auto_eval and auto_eval[criterion] is not None:
                    self.score += 1
                    print(f"‚úÖ Auto-√©valuation: {criterion}")
                else:
                    self.errors.append(f"‚ùå Auto-√©valuation manquante: {criterion}")
            
            # Validation des scores (doivent √™tre num√©riques)
            score_fields = ["analyse_ux_score", "conception_score", "prototypage_score", "satisfaction_globale"]
            for field in score_fields:
                if field in auto_eval:
                    try:
                        score_val = float(auto_eval[field])
                        if 0 <= score_val <= 5:  # √âchelle 0-5
                            self.score += 1
                            print(f"‚úÖ Score valide: {field}")
                        else:
                            self.warnings.append(f"‚ö†Ô∏è Score hors √©chelle (0-5): {field}")
                    except (ValueError, TypeError):
                        self.warnings.append(f"‚ö†Ô∏è Score non num√©rique: {field}")
        
        except Exception as e:
            self.errors.append(f"‚ùå Erreur auto-√©valuation: {e}")
        
        print(f"üìä Score auto-√©valuation: {self.score}/{self.max_score}")
        return self.score >= 6  # Minimum 60%

    def run_test(self, test_name):
        """Ex√©cute un test sp√©cifique"""
        test_methods = {
            "structure": self.test_json_structure,
            "ux_content": self.test_ux_content,
            "figma_links": self.test_figma_links,
            "qcm_answers": self.test_qcm_answers,
            "self_evaluation": self.test_self_evaluation
        }
        
        if test_name not in test_methods:
            print(f"‚ùå Test inconnu: {test_name}")
            return False
        
        try:
            success = test_methods[test_name]()
            
            # Affichage r√©sum√©
            if self.errors:
                print(f"\n‚ùå Erreurs ({len(self.errors)}):")
                for error in self.errors:
                    print(f"   {error}")
            
            if self.warnings:
                print(f"\n‚ö†Ô∏è Avertissements ({len(self.warnings)}):")
                for warning in self.warnings:
                    print(f"   {warning}")
            
            if success:
                print(f"\n‚úÖ Test {test_name} R√âUSSI")
            else:
                print(f"\n‚ùå Test {test_name} √âCHOU√â")
            
            return success
            
        except Exception as e:
            print(f"\nüí• Erreur test {test_name}: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Tests Atelier Figma TechSolutions")
    parser.add_argument("--test", required=True, 
                       choices=["structure", "ux_content", "figma_links", "qcm_answers", "self_evaluation"],
                       help="Type de test √† ex√©cuter")
    
    args = parser.parse_args()
    
    tester = FigmaWorkshopTester()
    success = tester.run_test(args.test)
    
    # Exit code pour GitHub Actions
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()